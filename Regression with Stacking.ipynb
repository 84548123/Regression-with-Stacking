{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMGKQpS819WmGxLh5BHhJOp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["---\n","\n","## **Machine Learning - II**\n","\n","---\n","\n","### **Stacked Regression Model Comparison**\n"],"metadata":{"id":"VeID6bYofkAx"}},{"cell_type":"code","source":["# Import necessary libraries\n","from numpy import mean  # To calculate the average of cross-validation scores\n","from sklearn.datasets import make_regression  # To generate a synthetic regression dataset\n","from sklearn.model_selection import cross_val_score, RepeatedKFold  # For cross-validation and model evaluation\n","from sklearn.linear_model import LinearRegression  # Meta-model for stacking\n","from sklearn.neighbors import KNeighborsRegressor  # Base model 1 (K-Nearest Neighbors)\n","from sklearn.tree import DecisionTreeRegressor  # Base model 2 (Decision Tree)\n","from sklearn.svm import SVR  # Base model 3 (Support Vector Regressor)\n","from sklearn.ensemble import StackingRegressor  # For building the Stacking ensemble model"],"metadata":{"id":"koUJK5MCPipI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a synthetic regression dataset\n","X, y = make_regression(n_samples=1000, n_features=20, random_state=1) # 1000 samples, 20 I/P values, to save the random values\n","# X: Feature matrix with 1000 samples and 20 features\n","# y: Target vector with 1000 values corresponding to each sample"],"metadata":{"id":"b0ujYw87PuT4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define a function to create a stacking model\n","def get_stacking():\n","    level0 = list()  # Initialize an empty list to hold base models (level 0)\n","    level0.append(('knn', KNeighborsRegressor()))  # Add KNN as a base model\n","    level0.append(('cart', DecisionTreeRegressor()))  # Add Decision Tree as a base model\n","    level0.append(('svm', SVR()))  # Add Support Vector Regressor as a base model\n","    level1 = LinearRegression()  # Define Linear Regression as the meta-model (level 1)\n","    model = StackingRegressor(estimators=level0, final_estimator=level1)  # Combine base models and meta-model in a Stacking Regressor\n","    return model  # Return the complete stacking model\n","\n","# Level 0 is base models\n","# Level 1 is meta model\n","# Level 0 is list due to it having multiple models in it"],"metadata":{"id":"oyTWZdEWPuRo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define a function to retrieve the models for comparison\n","def get_models():\n","    models = dict()  # Initialize an empty dictionary to store models\n","    models['knn'] = KNeighborsRegressor()  # Add KNN model to the dictionary\n","    models['cart'] = DecisionTreeRegressor()  # Add Decision Tree model to the dictionary\n","    models['svm'] = SVR()  # Add SVR model to the dictionary\n","    models['stacking'] = get_stacking()  # Add the stacking model to the dictionary\n","    return models  # Return the dictionary of models"],"metadata":{"id":"UTnEpONxPuPR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define a function to evaluate a model using cross-validation\n","def evaluate_model(model, X, y):\n","    cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)  # 10-fold cross-validation repeated 3 times\n","    # This divides the dataset(1000 samples) into 10 splits and repeats the process 3 times for reliable results\n","    scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv)  # Evaluate using negative MAE\n","    # Negative mean absolute error is used here because cross_val_score expects higher scores to be better\n","    return scores  # Return the cross-validation scores"],"metadata":{"id":"ZF_oum5jPuHg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get the dictionary of models (KNN, CART, SVR, Stacking)\n","models = get_models()\n","\n","# Initialize empty lists to store results and model names\n","results, names = list(), list()\n","\n","# Loop through each model in the dictionary\n","for name, model in models.items():\n","    scores = evaluate_model(model, X, y)  # Evaluate the model on the dataset using cross-validation\n","    results.append(scores)  # Append the scores to the results list\n","    names.append(name)  # Append the model name to the names list\n","    print(f'{name}: {mean(scores)}')  # Print the model name and the mean cross-validation score (mean of 30 scores)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BeummstwSlzf","executionInfo":{"status":"ok","timestamp":1727170980475,"user_tz":-330,"elapsed":14952,"user":{"displayName":"Joshua Dias","userId":"00235994748856120834"}},"outputId":"6f45fbf9-5a5e-489c-c770-13890b2369ee"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["knn -72.38368150286077\n","cart -89.59476145848235\n","svm -113.9538393363414\n","stacking -40.600319718110576\n"]}]},{"cell_type":"markdown","source":["### **Interpretation of Results:**\n","We used four different models to perform regression on a synthetic dataset: `KNeighborsRegressor` (KNN), `DecisionTreeRegressor` (CART), `SVR`, and a `StackingRegressor`. The evaluation was done using cross-validation, and the negative mean absolute error (NMAE) was used as the metric.\n","\n","Here are the results for each model:\n","- **KNN (KNeighborsRegressor):** NMAE = `-72.38`\n","- **CART (DecisionTreeRegressor):** NMAE = `-89.59`\n","- **SVR (Support Vector Regressor):** NMAE = `-113.95`\n","- **Stacking Model:** NMAE = `-40.60`\n","\n","### Interpretation:\n","1. **KNN:** The KNeighborsRegressor model had a moderate performance with an NMAE of `-72.38`. It performed better than both the CART and SVR models, but there was still room for improvement.\n","   \n","2. **CART:** The DecisionTreeRegressor performed worse than KNN, yielding an NMAE of `-89.59`. This model tends to overfit the training data, especially on small or noisy datasets, and its performance was suboptimal.\n","   \n","3. **SVR:** The SVR model had the poorest performance with an NMAE of `-113.95`. SVR models can struggle when data is high-dimensional or when parameters are not well-tuned, which could explain its poor performance here.\n","   \n","4. **Stacking Model:** The stacking model achieved the best performance with an NMAE of `-40.60`. By combining the predictions of multiple models (KNN, CART, SVR) and using a meta-model (LinearRegression) to make final predictions, the stacking approach reduced the error significantly.\n","\n","### Why Stacking Was Done:\n","The performance of individual base models was not satisfactory:\n","- **KNN** had moderate performance but could be improved.\n","- **CART** and **SVR** struggled to capture the relationships in the data well.\n","- No single model provided an optimal solution on its own.\n","\n","Stacking was used to **leverage the strengths** of each base model and create a more powerful ensemble. The idea behind stacking is that by combining multiple weak models, we can reduce their individual weaknesses and create a better final prediction.\n","\n","In this case, the **StackingRegressor** used the combined knowledge from KNN, DecisionTree, and SVR models, and then the meta-model (LinearRegression) learned how to combine their predictions effectively. This approach improved the overall accuracy and reduced the error, as evident from the best NMAE score of `-40.60`.\n","\n","### Conclusion:\n","The results demonstrate that stacking is an effective method to improve performance, especially when base models alone are insufficient. The stacking model outperformed individual models by a large margin, making it a superior choice for this regression task. The **combination of multiple models** helped generalize better to the underlying data, reducing the prediction error and yielding more reliable results."],"metadata":{"id":"pyJOfwTDkDx-"}}]}